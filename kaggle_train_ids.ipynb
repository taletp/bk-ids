{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e134f3c",
   "metadata": {},
   "source": [
    "# ⚠️ IMPORTANT: Feature Compatibility Note\n",
    "\n",
    "**The Challenge:**\n",
    "- **CIC-IDS2018** uses flow-based features (aggregated statistics over multiple packets)\n",
    "- **Live Capture** extracts packet-based features (attributes from individual packets)\n",
    "- These are fundamentally different feature spaces!\n",
    "\n",
    "**Our Approach:**\n",
    "This notebook selects 17 features from CIC-IDS2018 that have the **closest semantic meaning** to what can be extracted from live packets:\n",
    "\n",
    "**Training Features (CIC-IDS2018):**\n",
    "- Port numbers, protocol, packet lengths\n",
    "- TCP flags (SYN, ACK, FIN, RST, PSH, URG)\n",
    "- Packet rates, header lengths, traffic ratios\n",
    "\n",
    "**Live Capture Features (preprocessor.py):**\n",
    "- src_ip, dst_ip, src_port, dst_port\n",
    "- total_length, payload_size, ttl\n",
    "- tcp_syn_flag, tcp_ack_flag, tcp_fin_flag, tcp_rst_flag\n",
    "- window_size, sequence_number, packet_rate\n",
    "\n",
    "**Important Notes:**\n",
    "1. Features won't match exactly - this is an **approximation**\n",
    "2. Model learns attack patterns from statistical features\n",
    "3. After training, you'll need to create a **feature mapping layer** in your deployment\n",
    "4. Alternative: Implement flow tracking to compute CIC-IDS2018 features in real-time\n",
    "\n",
    "**For Production Use:**\n",
    "Consider training on synthetic data that matches your exact live capture features, or implement a full flow tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb786b7",
   "metadata": {},
   "source": [
    "# IDS/IPS Training on Kaggle with CSE-CIC-IDS2018 Dataset\n",
    "\n",
    "This notebook trains an Intrusion Detection System using the CSE-CIC-IDS2018 dataset on Kaggle infrastructure.\n",
    "\n",
    "**Features:**\n",
    "- Load CIC-IDS2018 dataset from Kaggle Datasets\n",
    "- Efficient memory management for large datasets\n",
    "- Advanced preprocessing with feature selection and balancing\n",
    "- Train multiple architectures (MLP, Random Forest, XGBoost)\n",
    "- Save models for deployment\n",
    "\n",
    "**Dataset:** [CSE-CIC-IDS2018](https://www.kaggle.com/datasets/solarmainframe/ids-intrusion-csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba688a01",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "Install packages not pre-installed on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c739ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install dependencies (if not already installed)\n",
    "!pip install -q imbalanced-learn\n",
    "!pip install -q xgboost\n",
    "!pip install -q lightgbm\n",
    "\n",
    "print(\"✓ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a289a8e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# TensorFlow (if available)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"✓ TensorFlow {tf.__version__} available\")\n",
    "except:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"⚠ TensorFlow not available, will use traditional ML models\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "if TF_AVAILABLE:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"✓ GPU detected: {len(gpus)} device(s)\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  - {gpu}\")\n",
    "            # Enable memory growth to avoid OOM\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"⚠ No GPU detected, using CPU\")\n",
    "else:\n",
    "    print(\"⚠ TensorFlow not available\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff6ad3",
   "metadata": {},
   "source": [
    "## 3. Configure Kaggle Environment\n",
    "\n",
    "Set up paths and check available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle paths\n",
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "KAGGLE_WORKING = Path('/kaggle/working')\n",
    "\n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = KAGGLE_INPUT.exists()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"✓ Running on Kaggle\")\n",
    "    print(f\"Input directory: {KAGGLE_INPUT}\")\n",
    "    print(f\"Working directory: {KAGGLE_WORKING}\")\n",
    "    \n",
    "    # List available datasets\n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for item in KAGGLE_INPUT.iterdir():\n",
    "        print(f\"  - {item.name}\")\n",
    "        # List CSV files in dataset\n",
    "        if item.is_dir():\n",
    "            csv_files = list(item.glob('*.csv'))\n",
    "            if csv_files:\n",
    "                print(f\"    CSV files found: {len(csv_files)}\")\n",
    "                for csv in csv_files[:3]:  # Show first 3\n",
    "                    print(f\"      • {csv.name}\")\n",
    "else:\n",
    "    print(\"⚠ Not running on Kaggle - using local paths\")\n",
    "    KAGGLE_INPUT = Path('../data/cic-ids2018')\n",
    "    KAGGLE_WORKING = Path('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de9636",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "**Dataset:** CSE-CIC-IDS2018 from Kaggle Datasets\n",
    "\n",
    "Add the dataset to your Kaggle notebook:\n",
    "1. Go to \"Add Data\" → Search \"ids-intrusion-csv\"\n",
    "2. Add \"solarmainframe/ids-intrusion-csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c650df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dataset directory\n",
    "dataset_dirs = list(KAGGLE_INPUT.glob('*ids*')) + list(KAGGLE_INPUT.glob('*intrusion*'))\n",
    "\n",
    "if dataset_dirs:\n",
    "    DATA_DIR = dataset_dirs[0]\n",
    "    print(f\"✓ Dataset found: {DATA_DIR}\")\n",
    "else:\n",
    "    # Fallback: list all and let user choose\n",
    "    print(\"Available directories:\")\n",
    "    for d in KAGGLE_INPUT.iterdir():\n",
    "        if d.is_dir():\n",
    "            print(f\"  {d.name}\")\n",
    "    DATA_DIR = KAGGLE_INPUT / 'ids-intrusion-csv'  # Default\n",
    "\n",
    "print(f\"\\nUsing data directory: {DATA_DIR}\")\n",
    "\n",
    "# List CSV files\n",
    "csv_files = sorted(list(DATA_DIR.glob('*.csv')))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  • {f.name:20s} ({size_mb:>6.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587245d",
   "metadata": {},
   "source": [
    "## 5. Define Configuration & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10109490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'use_simplified': True,  # Use simplified attack categories\n",
    "    'balance_method': 'hybrid',  # 'hybrid', 'undersample', 'oversample', 'smote', or None\n",
    "    'min_samples_per_class': 10000,  # Minimum samples to keep per class (for hybrid)\n",
    "    'max_samples_per_class': 100000,  # Maximum samples to keep per class (for hybrid)\n",
    "    'test_size': 0.2,\n",
    "    'scaler_type': 'robust',  # 'standard', 'minmax', 'robust'\n",
    "    'correlation_threshold': 0.9,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Which days to load (None = all, or list specific files)\n",
    "    'days_to_load': [\n",
    "        # '02-15-2018.csv',\n",
    "        # '02-16-2018.csv',\n",
    "        # '02-20-2018.csv',\n",
    "        # '02-21-2018.csv',\n",
    "        # '02-22-2018.csv',\n",
    "        # '02-23-2018.csv',\n",
    "    ],\n",
    "    \n",
    "    # Model training\n",
    "    'model_type': 'random_forest',  # 'random_forest', 'xgboost', 'lightgbm', 'mlp'\n",
    "    'epochs': 50,  # For neural networks\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Attack class mappings\n",
    "SIMPLIFIED_MAPPING = {\n",
    "    'Benign': 'Normal',\n",
    "    'FTP-BruteForce': 'BruteForce',\n",
    "    'SSH-Bruteforce': 'BruteForce',\n",
    "    'DoS-GoldenEye': 'DoS',\n",
    "    'DoS-Slowloris': 'DoS',\n",
    "    'DoS-SlowHTTPTest': 'DoS',\n",
    "    'DoS-Hulk': 'DoS',\n",
    "    'Heartbleed': 'Exploit',\n",
    "    'Web-BruteForce': 'Web',\n",
    "    'Web-XSS': 'Web',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Botnet': 'Botnet',\n",
    "    'DDoS-LOIC-HTTP': 'DDoS',\n",
    "    'DDoS-HOIC': 'DDoS',\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Days to load: {len(CONFIG['days_to_load'])}\")\n",
    "print(f\"  Simplified labels: {CONFIG['use_simplified']}\")\n",
    "print(f\"  Balance method: {CONFIG['balance_method']}\")\n",
    "print(f\"  Model: {CONFIG['model_type']}\")\n",
    "if CONFIG['balance_method'] == 'hybrid':\n",
    "    print(f\"  Min samples/class: {CONFIG['min_samples_per_class']:,}\")\n",
    "    print(f\"  Max samples/class: {CONFIG['max_samples_per_class']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_day(csv_path, chunksize=50000):\n",
    "    \"\"\"Load and preprocess a single day's data with memory optimization\"\"\"\n",
    "    print(f\"\\nLoading {csv_path.name}...\")\n",
    "    \n",
    "    # Drop unnecessary columns during read\n",
    "    drop_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp']\n",
    "    \n",
    "    # Load in chunks to reduce memory\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunksize, low_memory=False):\n",
    "        # Drop unnecessary columns\n",
    "        chunk = chunk.drop(columns=[col for col in drop_cols if col in chunk.columns], errors='ignore')\n",
    "        \n",
    "        # Remove duplicate header rows\n",
    "        if 'Dst Port' in chunk.columns:\n",
    "            chunk = chunk[chunk['Dst Port'] != 'Dst Port']\n",
    "        \n",
    "        # Handle infinity and null\n",
    "        chunk = chunk.replace([\"Infinity\", \"infinity\"], np.inf)\n",
    "        chunk = chunk.replace([np.inf, -np.inf], np.nan)\n",
    "        chunk.dropna(inplace=True)\n",
    "        \n",
    "        # Convert numeric columns to float32 to save memory\n",
    "        for col in chunk.columns:\n",
    "            if col != 'Label':\n",
    "                chunk[col] = pd.to_numeric(chunk[col], errors='coerce', downcast='float')\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Combine chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"  Loaded {len(df):,} samples\")\n",
    "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"  Attack distribution: {df['Label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from selected days using generator\n",
    "def load_days_generator():\n",
    "    \"\"\"Generator to load days one at a time\"\"\"\n",
    "    for day in CONFIG['days_to_load']:\n",
    "        day_path = DATA_DIR / day\n",
    "        if day_path.exists():\n",
    "            yield load_and_preprocess_day(day_path)\n",
    "        else:\n",
    "            print(f\"⚠ {day} not found, skipping...\")\n",
    "\n",
    "# Combine all days\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Combining data from all days...\")\n",
    "df = pd.concat(load_days_generator(), ignore_index=True)\n",
    "gc.collect()  # Force garbage collection\n",
    "\n",
    "# Optimize data types after combining\n",
    "for col in df.columns:\n",
    "    if col != 'Label':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
    "\n",
    "print(f\"✓ Total samples: {len(df):,}\")\n",
    "print(f\"✓ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ca5e6",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d172d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 17 packet-level features compatible with live capture\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE SELECTION: Mapping to 17 packet-level features\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n⚠️  IMPORTANT: Selecting features that match preprocessor.py\")\n",
    "print(\"Live capture extracts: IPs, ports, length, TTL, flags, payload size, etc.\\n\")\n",
    "\n",
    "# Map CIC-IDS2018 features to closest preprocessor equivalents\n",
    "# Preprocessor extracts from single packets:\n",
    "# src_ip_numeric, dst_ip_numeric, total_length, fragment_offset, is_fragment,\n",
    "# payload_size, ttl, src_port, dst_port, tcp_syn_flag, tcp_ack_flag,\n",
    "# tcp_fin_flag, tcp_rst_flag, window_size, sequence_number, udp_length, packet_rate\n",
    "\n",
    "FEATURE_MAPPING = {\n",
    "    # Direct matches or close approximations\n",
    "    'Dst Port': 'dst_port',              # ✓ Direct match\n",
    "    'Protocol': 'protocol',              # ✓ Direct match (6=TCP, 17=UDP, 1=ICMP)\n",
    "    'TotLen Fwd Pkts': 'total_length',   # ~ Total length of packets\n",
    "    'Fwd Pkt Len Max': 'payload_size',   # ~ Max payload approximates payload size\n",
    "    'Fwd Pkt Len Mean': 'mean_length',   # ~ Mean packet length\n",
    "    'Flow Byts/s': 'packet_rate',        # ~ Bytes per second approximates rate\n",
    "    \n",
    "    # TCP Flags (can be extracted from single packets)\n",
    "    'PSH Flag Cnt': 'tcp_psh_flag',      # TCP PSH flag\n",
    "    'URG Flag Cnt': 'tcp_urg_flag',      # TCP URG flag  \n",
    "    'FIN Flag Cnt': 'tcp_fin_flag',      # ✓ Direct match\n",
    "    'SYN Flag Cnt': 'tcp_syn_flag',      # ✓ Direct match\n",
    "    'RST Flag Cnt': 'tcp_rst_flag',      # ✓ Direct match\n",
    "    'ACK Flag Cnt': 'tcp_ack_flag',      # ✓ Direct match\n",
    "    \n",
    "    # Header and size features\n",
    "    'Fwd Header Len': 'header_length',   # Header length\n",
    "    'Fwd Pkt Len Min': 'min_length',     # Min packet length\n",
    "    'Fwd Pkt Len Std': 'length_std',     # Packet length variation\n",
    "    \n",
    "    # Additional useful features\n",
    "    'Down/Up Ratio': 'down_up_ratio',    # Traffic ratio\n",
    "    'Fwd Pkts/s': 'packet_rate_fwd',     # Packet rate\n",
    "}\n",
    "\n",
    "# Try to select features in order of preference\n",
    "selected_features = []\n",
    "selected_cic_names = []\n",
    "\n",
    "print(\"Searching for compatible features...\\n\")\n",
    "\n",
    "# First, try to get all mapped features\n",
    "for cic_feat, preprocessor_equiv in FEATURE_MAPPING.items():\n",
    "    if cic_feat in df.columns:\n",
    "        selected_cic_names.append(cic_feat)\n",
    "        print(f\"  ✓ {cic_feat:25s} → {preprocessor_equiv}\")\n",
    "\n",
    "# If we have less than 17, add more features\n",
    "if len(selected_cic_names) < 17:\n",
    "    print(f\"\\nNeed {17 - len(selected_cic_names)} more features...\")\n",
    "    \n",
    "    # Additional candidates that might be useful\n",
    "    backup_features = [\n",
    "        'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Bwd Pkts',\n",
    "        'Bwd Pkt Len Mean', 'Flow Duration', 'Flow IAT Mean',\n",
    "        'Fwd IAT Mean', 'Pkt Len Mean', 'Pkt Len Std',\n",
    "        'Pkt Len Var', 'Fwd Seg Size Avg', 'Init Fwd Win Byts'\n",
    "    ]\n",
    "    \n",
    "    for feat in backup_features:\n",
    "        if feat in df.columns and feat not in selected_cic_names:\n",
    "            selected_cic_names.append(feat)\n",
    "            print(f\"  + {feat:25s} (backup feature)\")\n",
    "            if len(selected_cic_names) >= 17:\n",
    "                break\n",
    "\n",
    "# Limit to exactly 17\n",
    "selected_cic_names = selected_cic_names[:17]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"✓ Final selection: {len(selected_cic_names)} features\")\n",
    "print(f\"{'='*50}\")\n",
    "for i, feat in enumerate(selected_cic_names, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Filter dataframe to only include these features + Label\n",
    "df_filtered = df[selected_cic_names + ['Label']].copy()\n",
    "\n",
    "# Save feature names globally for metadata\n",
    "SELECTED_FEATURE_NAMES = selected_cic_names\n",
    "\n",
    "# Replace original dataframe\n",
    "del df\n",
    "df = df_filtered\n",
    "del df_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Filtered dataset shape: {df.shape}\")\n",
    "print(f\"✓ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af351041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify attack classes if configured\n",
    "if CONFIG['use_simplified']:\n",
    "    print(\"Applying simplified class mapping...\")\n",
    "    df['Label'] = df['Label'].replace(SIMPLIFIED_MAPPING)\n",
    "    print(f\"✓ Classes reduced to: {df['Label'].nunique()} unique values\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "del df  # Free memory immediately\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"✓ Number of features: {X.shape[1]} (target: 17 for live capture compatibility)\")\n",
    "print(f\"Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric with memory-efficient types\n",
    "print(\"Converting features to numeric...\")\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce', downcast='float')\n",
    "\n",
    "# Handle any remaining NaNs\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Remove constant and near-constant features\n",
    "print(\"\\nRemoving constant features...\")\n",
    "variances = X.var()\n",
    "constant_features = variances[variances < 1e-8].index.tolist()\n",
    "if constant_features:\n",
    "    X = X.drop(columns=constant_features)\n",
    "    print(f\"  Dropped {len(constant_features)} constant features\")\n",
    "\n",
    "del variances, constant_features\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Final feature matrix: {X.shape}\")\n",
    "print(f\"✓ Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a8b20",
   "metadata": {},
   "source": [
    "## 7. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes before splitting\n",
    "balance_method = CONFIG.get('balance_method', 'hybrid')\n",
    "print(f\"Applying {balance_method} balancing...\")\n",
    "\n",
    "if balance_method == 'hybrid':\n",
    "    # Smart balancing: oversample minorities, cap majorities\n",
    "    min_samples = CONFIG.get('min_samples_per_class', 10000)\n",
    "    max_samples = CONFIG.get('max_samples_per_class', 100000)\n",
    "    \n",
    "    print(f\"  Target range: {min_samples:,} - {max_samples:,} samples per class\")\n",
    "    \n",
    "    # Calculate current distribution\n",
    "    class_counts = y.value_counts()\n",
    "    print(f\"\\nOriginal distribution:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  {cls}: {count:,}\")\n",
    "    \n",
    "    # Build sampling strategy\n",
    "    sampling_strategy = {}\n",
    "    for cls, count in class_counts.items():\n",
    "        if count < min_samples:\n",
    "            # Oversample minority classes\n",
    "            sampling_strategy[cls] = min_samples\n",
    "        elif count > max_samples:\n",
    "            # Undersample majority classes\n",
    "            sampling_strategy[cls] = max_samples\n",
    "        else:\n",
    "            # Keep as is\n",
    "            sampling_strategy[cls] = count\n",
    "    \n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    for cls, target in sampling_strategy.items():\n",
    "        original = class_counts[cls]\n",
    "        change = \"↑ oversample\" if target > original else \"↓ undersample\" if target < original else \"→ unchanged\"\n",
    "        print(f\"  {cls}: {original:,} → {target:,} {change}\")\n",
    "    \n",
    "    # First oversample minorities\n",
    "    minority_classes = [cls for cls, count in class_counts.items() if count < min_samples]\n",
    "    if minority_classes:\n",
    "        print(f\"\\nOversampling {len(minority_classes)} minority classes...\")\n",
    "        oversample_strategy = {cls: sampling_strategy[cls] for cls in minority_classes}\n",
    "        oversampler = RandomOverSampler(sampling_strategy=oversample_strategy, random_state=42)\n",
    "        X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "    \n",
    "    # Then undersample majorities\n",
    "    majority_classes = [cls for cls, count in class_counts.items() if count > max_samples]\n",
    "    if majority_classes:\n",
    "        print(f\"Undersampling {len(majority_classes)} majority classes...\")\n",
    "        undersample_strategy = {cls: sampling_strategy[cls] for cls in majority_classes}\n",
    "        undersampler = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "        X_balanced, y_balanced = undersampler.fit_resample(X_resampled, y_resampled)\n",
    "    else:\n",
    "        X_balanced, y_balanced = X_resampled, y_resampled\n",
    "    \n",
    "    print(f\"\\n✓ Before: {X.shape[0]:,} samples\")\n",
    "    print(f\"✓ After: {X_balanced.shape[0]:,} samples\")\n",
    "    print(f\"\\nFinal balanced distribution:\")\n",
    "    print(pd.Series(y_balanced).value_counts())\n",
    "    \n",
    "elif balance_method == 'undersample':\n",
    "    # Custom undersampling with minimum threshold\n",
    "    min_samples = CONFIG.get('min_samples_per_class', 10000)\n",
    "    class_counts = y.value_counts()\n",
    "    \n",
    "    # Set all classes to max(minority_class_size, min_samples)\n",
    "    target_size = max(class_counts.min(), min_samples)\n",
    "    print(f\"  Target samples per class: {target_size:,}\")\n",
    "    \n",
    "    sampler = RandomUnderSampler(sampling_strategy={cls: min(count, target_size) for cls, count in class_counts.items()}, random_state=42)\n",
    "    X_balanced, y_balanced = sampler.fit_resample(X, y)\n",
    "    print(f\"✓ Before: {X.shape[0]:,} samples\")\n",
    "    print(f\"✓ After: {X_balanced.shape[0]:,} samples\")\n",
    "    print(f\"\\nBalanced distribution:\")\n",
    "    print(pd.Series(y_balanced).value_counts())\n",
    "    \n",
    "elif balance_method == 'oversample':\n",
    "    sampler = RandomOverSampler(random_state=42)\n",
    "    X_balanced, y_balanced = sampler.fit_resample(X, y)\n",
    "    print(f\"✓ Before: {X.shape[0]:,} samples\")\n",
    "    print(f\"✓ After: {X_balanced.shape[0]:,} samples\")\n",
    "    print(f\"\\nBalanced distribution:\")\n",
    "    print(pd.Series(y_balanced).value_counts())\n",
    "    \n",
    "elif balance_method == 'smote':\n",
    "    sampler = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_balanced, y_balanced = sampler.fit_resample(X, y)\n",
    "    print(f\"✓ Before: {X.shape[0]:,} samples\")\n",
    "    print(f\"✓ After: {X_balanced.shape[0]:,} samples\")\n",
    "    print(f\"\\nBalanced distribution:\")\n",
    "    print(pd.Series(y_balanced).value_counts())\n",
    "    \n",
    "else:\n",
    "    X_balanced, y_balanced = X, y\n",
    "    print(\"✓ No balancing applied\")\n",
    "\n",
    "# Convert back to float32 if needed\n",
    "if isinstance(X_balanced, pd.DataFrame):\n",
    "    for col in X_balanced.columns:\n",
    "        X_balanced[col] = X_balanced[col].astype('float32')\n",
    "\n",
    "# Free memory\n",
    "del X, y\n",
    "gc.collect()\n",
    "print(f\"\\n✓ Memory after balancing: {X_balanced.memory_usage(deep=True).sum() / 1024**2:.1f} MB\" if isinstance(X_balanced, pd.DataFrame) else f\"✓ Memory after balancing: {X_balanced.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee5fda",
   "metadata": {},
   "source": [
    "## 8. Train-Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_balanced)\n",
    "\n",
    "print(f\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"✓ Test: {X_test.shape[0]:,} samples\")\n",
    "print(f\"✓ Features: {X_train.shape[1]} (compatible with 17-feature live capture)\")\n",
    "\n",
    "# Save feature names before deleting\n",
    "feature_names = SELECTED_FEATURE_NAMES if 'SELECTED_FEATURE_NAMES' in globals() else (X_train.columns.tolist() if hasattr(X_train, 'columns') else [])\n",
    "\n",
    "del X_balanced, y_balanced, y_encoded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (convert to float32 to save memory)\n",
    "print(\"Scaling features with StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train).astype('float32')\n",
    "X_test_scaled = scaler.transform(X_test).astype('float32')\n",
    "\n",
    "# Delete unscaled versions\n",
    "del X_train, X_test\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Scaled training data: {X_train_scaled.shape}\")\n",
    "print(f\"✓ Scaled test data: {X_test_scaled.shape}\")\n",
    "print(f\"✓ Train memory: {X_train_scaled.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"✓ Test memory: {X_test_scaled.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08473fb0",
   "metadata": {},
   "source": [
    "## 9. Model Training\n",
    "\n",
    "Train models based on CONFIG['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = CONFIG['model_type']\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Training {model_type.upper()} model for {n_classes} classes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if model_type == 'random_forest':\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "elif model_type == 'xgboost':\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    # Check for GPU\n",
    "    try:\n",
    "        import subprocess\n",
    "        gpu_available = subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0\n",
    "    except:\n",
    "        gpu_available = False\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 1\n",
    "    }\n",
    "    \n",
    "    if gpu_available:\n",
    "        # XGBoost 3.1+ uses 'device' instead of 'gpu_id'\n",
    "        xgb_params['device'] = 'cuda'\n",
    "        print(\"  Using GPU acceleration for XGBoost\")\n",
    "    else:\n",
    "        xgb_params['device'] = 'cpu'\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)])\n",
    "\n",
    "elif model_type == 'lightgbm':\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    # Check for GPU\n",
    "    try:\n",
    "        import subprocess\n",
    "        gpu_available = subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0\n",
    "    except:\n",
    "        gpu_available = False\n",
    "    \n",
    "    lgb_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    if gpu_available:\n",
    "        lgb_params['device'] = 'gpu'\n",
    "        print(\"  Using GPU acceleration for LightGBM\")\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)])\n",
    "    \n",
    "elif model_type == 'mlp':\n",
    "    if not TF_AVAILABLE:\n",
    "        raise RuntimeError(\"TensorFlow not available for MLP training\")\n",
    "    \n",
    "    # TensorFlow MLP\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02f4d8",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab846934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# For neural networks, get class predictions\n",
    "if model_type == 'mlp':\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n✓ Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n✓ Confusion Matrix shape: {cm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3982381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {model_type.upper()}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(KAGGLE_WORKING / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046e75f",
   "metadata": {},
   "source": [
    "## 11. Save Model & Artifacts\n",
    "\n",
    "Save trained model, scaler, and metadata for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "if model_type == 'mlp':\n",
    "    model_path = KAGGLE_WORKING / 'ids_model_mlp.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"✓ Keras model saved to {model_path}\")\n",
    "else:\n",
    "    model_path = KAGGLE_WORKING / f'ids_model_{model_type}.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ {model_type.upper()} model saved to {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = KAGGLE_WORKING / 'scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save label encoder\n",
    "encoder_path = KAGGLE_WORKING / 'label_encoder.joblib'\n",
    "joblib.dump(label_encoder, encoder_path)\n",
    "print(f\"✓ Label encoder saved to {encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'model_type': model_type,\n",
    "    'n_classes': n_classes,\n",
    "    'class_names': label_encoder.classes_.tolist(),\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': X_train_scaled.shape[1],\n",
    "    'accuracy': float(accuracy),\n",
    "    'training_samples': X_train_scaled.shape[0],\n",
    "    'test_samples': X_test_scaled.shape[0],\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "metadata_path = KAGGLE_WORKING / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved to {metadata_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL ARTIFACTS SAVED TO /kaggle/working/\")\n",
    "print(\"Download these files to use in your local IDS system:\")\n",
    "print(\"  - Model file (.keras or .joblib)\")\n",
    "print(\"  - scaler.joblib\")\n",
    "print(\"  - label_encoder.joblib\")\n",
    "print(\"  - model_metadata.json\")\n",
    "print(\"  - confusion_matrix.png\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
